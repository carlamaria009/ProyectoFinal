!pip install tensorflow-addons
import os
import numpy as np
import joblib
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.metrics import classification_report
from models.metrics import metrics_values
from tensorflow_addons.losses import SigmoidFocalCrossEntropy
from keras.models import save_model

def run_mlp(
    data_path="src/",
    models_path="models/"
):
    # 📦 Cargar datos
    x_train, y_train, feature_names = joblib.load(os.path.join(data_path, "train.pkl"))
    x_val, y_val, _ = joblib.load(os.path.join(data_path, "val.pkl"))
    x_test, y_test, _ = joblib.load(os.path.join(data_path, "test.pkl"))

    # 🔁 Aplanar entradas si es necesario
    if len(x_train.shape) > 2:
        x_train = x_train.reshape(x_train.shape[0], -1)
        x_val = x_val.reshape(x_val.shape[0], -1)
        x_test = x_test.reshape(x_test.shape[0], -1)

    # 🏷️ Cargar nombres de clases
    class_labels_path = os.path.join(data_path, "class_labels.npy")
    if os.path.exists(class_labels_path):
        class_names = np.load(class_labels_path, allow_pickle=True).tolist()
        print("✅ Clases cargadas:", class_names)
    else:
        raise FileNotFoundError("❌ No se encontró el archivo class_labels.npy.")

    n_classes = y_train.shape[1]
    input_dim = x_train.shape[1]

    # 🎯 Convertir etiquetas one-hot a enteros para selección de características
    y_train_labels = np.argmax(y_train, axis=1)

    # ⭐ Aplicar selección de características
    from sklearn.feature_selection import SelectKBest, f_classif
    selector = SelectKBest(f_classif, k=200)
    x_train = selector.fit_transform(x_train, y_train_labels)
    x_val = selector.transform(x_val)
    x_test = selector.transform(x_test)

    # 🧠 Construir MLP
    model = models.Sequential([
        layers.Input(shape=(input_dim,)),
        layers.Dense(512, activation='relu'),
        #layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(256, activation='relu'),
        #layers.BatchNormalization(),
        layers.Dropout(0.2),
        layers.Dense(128, activation='relu'),
        #layers.Dropout(0.2),
        layers.Dense(n_classes, activation='softmax')
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
        loss=SigmoidFocalCrossEntropy(gamma=2.0),#'categorical_crossentropy',
        metrics=['accuracy']
    )

    # ⏱️ Callbacks
    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # 🚆 Entrenar
    print("🚀 Entrenando MLP...")
    history = model.fit(
        x_train, y_train,
        validation_data=(x_val, y_val),
        epochs=100,
        batch_size=32,
        callbacks=[early_stop],
        verbose=2
    )

    # 💾 Guardar modelo
    model_path = os.path.join(models_path, "mlp.h5")
    model.save(model_path)
    print(f"📦 Modelo MLP guardado en: {model_path}")

    # 🧪 Evaluación en test
    y_pred_probs = model.predict(x_test)
    y_pred_labels = np.argmax(y_pred_probs, axis=1)
    y_test_labels = np.argmax(y_test, axis=1)

    print("📈 Evaluación final en conjunto de prueba:")
    metrics_values(y_test_labels, y_pred_labels, class_names)

    return model, x_test, feature_names
